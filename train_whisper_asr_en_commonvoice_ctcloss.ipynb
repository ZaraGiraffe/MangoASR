{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T11:52:56.718576100Z",
     "start_time": "2024-02-01T11:52:53.944189400Z"
    }
   },
   "outputs": [],
   "source": [
    "import datasets, transformers, huggingface_hub as hub\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T12:07:03.497464900Z",
     "start_time": "2024-02-01T12:07:03.495955100Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T11:53:06.249247500Z",
     "start_time": "2024-02-01T11:53:06.233973600Z"
    }
   },
   "outputs": [],
   "source": [
    "import loaders\n",
    "from DatasetMixer import DatasetMixer, DatasetMixerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "outputs": [],
   "source": [
    "def get_common_voice(lang, streaming=False) -> datasets.DatasetDict:\n",
    "    \"\"\"\n",
    "    Loads Ukrainian Common Voice dataset from here\n",
    "    https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0\n",
    "    makes it compatible with DatasetMixer\n",
    "    :param lang: the language of common voice dataset. Should be in [\"en\", \"uk\"]\n",
    "    :param streaming (optional): pass this parameter to load_dataset function \n",
    "    \"\"\"\n",
    "    uk_speech_dataset = load_dataset(\"mozilla-foundation/common_voice_11_0\", lang, streaming=streaming, split=\"validation\")\n",
    "    uk_speech_dataset = uk_speech_dataset.rename_columns({\"client_id\": \"speaker_id\", \"sentence\": \"transcription\"})\n",
    "    return uk_speech_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T14:37:27.425199700Z",
     "start_time": "2024-02-01T14:37:27.403058300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T16:28:52.889465600Z",
     "start_time": "2024-02-01T16:28:48.105135900Z"
    }
   },
   "outputs": [],
   "source": [
    "speech_dataset = loaders.get_common_voice(\"en\", streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T16:28:57.534168600Z",
     "start_time": "2024-02-01T16:28:57.527164200Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T16:28:58.374476400Z",
     "start_time": "2024-02-01T16:28:58.368399300Z"
    }
   },
   "outputs": [],
   "source": [
    "speech_dataset = speech_dataset.cast_column(\"audio\", Audio(16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "outputs": [],
   "source": [
    "from transformers import WhisperFeatureExtractor, WhisperForConditionalGeneration"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T16:28:59.514025400Z",
     "start_time": "2024-02-01T16:28:59.492998500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "outputs": [],
   "source": [
    "class WhisperBaseEncoderForCTC(nn.Module):\n",
    "    def __init__(self, num_tokens, sampling_rate=16000):\n",
    "        super().__init__()\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.feature_extractor = WhisperFeatureExtractor(return_attention_mask=True, sampling_rate=16000)\n",
    "        self.whisper_encoder = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\").get_encoder()\n",
    "        self.linear = nn.Linear(512, num_tokens)\n",
    "    \n",
    "    def forward(self, X: list[np.ndarray], device=\"cuda\"):\n",
    "        features = self.feature_extractor(X, sampling_rate=self.sampling_rate)\n",
    "        features[\"input_features\"] = torch.tensor(np.stack(features[\"input_features\"])).to(device)\n",
    "        features[\"attention_mask\"] = torch.tensor(features[\"attention_mask\"]).to(device)\n",
    "        features_input_size = features[\"attention_mask\"].shape[1]\n",
    "        encoder_output = self.whisper_encoder(**features)\n",
    "        encoder_output_size = encoder_output.last_hidden_state.shape[1]\n",
    "        Y = self.linear(encoder_output.last_hidden_state)\n",
    "        mask = features[\"attention_mask\"][:,::features_input_size // encoder_output_size]\n",
    "        return Y.transpose(0, 1), mask"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T16:35:54.836730Z",
     "start_time": "2024-02-01T16:35:54.825552800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'input_features': [array([[-0.7125068 , -0.7125068 , -0.66656506, ..., -0.7125068 ,\n        -0.7125068 , -0.7125068 ],\n       [-0.7125068 , -0.7125068 , -0.6105869 , ..., -0.7125068 ,\n        -0.7125068 , -0.7125068 ],\n       [-0.7125068 , -0.7125068 , -0.4616264 , ..., -0.7125068 ,\n        -0.7125068 , -0.7125068 ],\n       ...,\n       [-0.7125068 , -0.7125068 , -0.42897058, ..., -0.7125068 ,\n        -0.7125068 , -0.7125068 ],\n       [-0.7125068 , -0.7125068 , -0.57720065, ..., -0.7125068 ,\n        -0.7125068 , -0.7125068 ],\n       [-0.7125068 , -0.7125068 , -0.5909358 , ..., -0.7125068 ,\n        -0.7125068 , -0.7125068 ]], dtype=float32)], 'attention_mask': array([[1, 1, 1, ..., 0, 0, 0]])}"
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WhisperFeatureExtractor(return_attention_mask=True)(example1[\"audio\"][\"array\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T17:14:03.998446200Z",
     "start_time": "2024-02-01T17:14:03.964484100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "outputs": [],
   "source": [
    "def generate(X: np.ndarray):\n",
    "    with torch.no_grad():\n",
    "        features_extraction = model.feature_extractor(X)\n",
    "        features = features_extraction[\"input_features\"][0]\n",
    "        features = torch.tensor(features)\n",
    "        attention_mask = features_extraction[\"attention_mask\"][0]\n",
    "        features_input_size = attention_mask.shape[0]\n",
    "        features = torch.unsqueeze(features, dim=0).cuda()\n",
    "        output = model.whisper_encoder(features).last_hidden_state\n",
    "        encoder_output_size = output.shape[1]\n",
    "        attention_length = torch.tensor(attention_mask[::features_input_size // encoder_output_size]).sum()\n",
    "        Y = model.linear(output)[0][:attention_length,:]\n",
    "        Y = torch.argmax(Y, dim=1).cpu().numpy()\n",
    "        return tokenizer.decode(Y)\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T17:22:43.599114300Z",
     "start_time": "2024-02-01T17:22:43.589598900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "data": {
      "text/plain": "'THE TR EA A P R S ON THE C OM P I L A T ION AL B O M C R E F OR K S <unk>'"
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(example1[\"audio\"][\"array\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T17:22:44.122006700Z",
     "start_time": "2024-02-01T17:22:43.757913100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "outputs": [
    {
     "data": {
      "text/plain": "'The track appears on the compilation album \"Kraftworks\".'"
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example1[\"transcription\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T17:23:20.495140900Z",
     "start_time": "2024-02-01T17:23:20.485041Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "outputs": [],
   "source": [
    "model = WhisperBaseEncoderForCTC(32).to(\"cuda\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T16:35:55.809412500Z",
     "start_time": "2024-02-01T16:35:54.997255Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T16:35:55.818002800Z",
     "start_time": "2024-02-01T16:35:55.809412500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "outputs": [],
   "source": [
    "tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\"facebook/wav2vec2-base-100h\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T16:35:56.150153700Z",
     "start_time": "2024-02-01T16:35:55.997424900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "outputs": [
    {
     "data": {
      "text/plain": "{\"'\": 27,\n '</s>': 2,\n '<pad>': 0,\n '<s>': 1,\n '<unk>': 3,\n 'A': 7,\n 'B': 24,\n 'C': 19,\n 'D': 14,\n 'E': 5,\n 'F': 20,\n 'G': 21,\n 'H': 11,\n 'I': 10,\n 'J': 29,\n 'K': 26,\n 'L': 15,\n 'M': 17,\n 'N': 9,\n 'O': 8,\n 'P': 23,\n 'Q': 30,\n 'R': 13,\n 'S': 12,\n 'T': 6,\n 'U': 16,\n 'V': 25,\n 'W': 18,\n 'X': 28,\n 'Y': 22,\n 'Z': 31,\n '|': 4}"
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T16:35:56.614350500Z",
     "start_time": "2024-02-01T16:35:56.605839Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "outputs": [],
   "source": [
    "from torch.nn.functional import ctc_loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T16:35:56.823448400Z",
     "start_time": "2024-02-01T16:35:56.822448400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "outputs": [],
   "source": [
    "def whisper_ctc_loss(logits, logits_mask, tokens, tokens_mask, blank):\n",
    "    log_softmax_vectors = torch.log_softmax(logits, dim=2)\n",
    "    logits_lengths = torch.sum(logits_mask, dim=1)\n",
    "    tokens = torch.tensor(tokens)\n",
    "    tokens_lengths = torch.sum(torch.tensor(tokens_mask), dim=1)\n",
    "    loss = ctc_loss(\n",
    "        log_probs=log_softmax_vectors,\n",
    "        targets=tokens,\n",
    "        input_lengths=logits_lengths,\n",
    "        target_lengths=tokens_lengths,\n",
    "        blank=blank,\n",
    "    )\n",
    "    return loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T16:35:57.174413Z",
     "start_time": "2024-02-01T16:35:57.168408400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "outputs": [],
   "source": [
    "def default_list_speech_collator(examples):\n",
    "    new_examples = {\n",
    "        \"arrays\": [examples[i][\"audio\"][\"array\"] for i in range(len(examples))],\n",
    "        \"transcriptions\": [examples[i][\"transcription\"] for i in range(len(examples))],\n",
    "        \"sampling_date\": examples[0][\"audio\"][\"sampling_rate\"],\n",
    "    }\n",
    "    return new_examples"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T16:35:57.371911400Z",
     "start_time": "2024-02-01T16:35:57.366907800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, default_collate"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T16:35:57.682747300Z",
     "start_time": "2024-02-01T16:35:57.676238900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "outputs": [],
   "source": [
    "loader = DataLoader(speech_dataset[\"train\"], batch_size=4, collate_fn=default_list_speech_collator)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T16:35:57.868510Z",
     "start_time": "2024-02-01T16:35:57.861408600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "outputs": [],
   "source": [
    "class TrainerBase:\n",
    "    def __init__(self, model, dataloader, optimizer):\n",
    "        self.model = model\n",
    "        self.dataloader = dataloader\n",
    "        self.optimizer = optimizer\n",
    "    \n",
    "    def loss(self, **kwargs):\n",
    "        raise NotImplemented\n",
    "    \n",
    "    def _prepare_model_input(self, batch):\n",
    "        raise NotImplemented\n",
    "    \n",
    "    def _prepare_loss_input(self, batch, model_output):\n",
    "        raise NotImplemented\n",
    "    \n",
    "    def train(self, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            losses = []\n",
    "            print(f\"EPOCH {epoch}\")\n",
    "            for i, batch in enumerate(self.dataloader):\n",
    "                model_input = self._prepare_model_input(batch)\n",
    "                model_output = self.model(**model_input)\n",
    "                loss_input = self._prepare_loss_input(batch, model_output)\n",
    "                loss = self.loss(**loss_input)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                print(f\"loss: {loss.clone().detach().cpu().numpy()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T16:39:10.373632100Z",
     "start_time": "2024-02-01T16:39:10.368626200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "outputs": [],
   "source": [
    "class WhisperTrainer(TrainerBase):\n",
    "    def __init__(self, model, dataloader, optimizer, tokenizer):\n",
    "        super().__init__(model, dataloader, optimizer)\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def loss(self, logits, logits_mask, tokens, tokens_mask, blank):\n",
    "        log_softmax_vectors = torch.log_softmax(logits, dim=2)\n",
    "        logits_lengths = torch.sum(logits_mask, dim=1)\n",
    "        tokens = torch.tensor(tokens)\n",
    "        tokens_lengths = torch.sum(torch.tensor(tokens_mask), dim=1)\n",
    "        loss = ctc_loss(\n",
    "            log_probs=log_softmax_vectors,\n",
    "            targets=tokens,\n",
    "            input_lengths=logits_lengths,\n",
    "            target_lengths=tokens_lengths,\n",
    "            blank=blank,\n",
    "            zero_infinity=True,\n",
    "        )\n",
    "        return loss\n",
    "    \n",
    "    def _prepare_model_input(self, batch):\n",
    "        return {\n",
    "            \"X\": batch[\"arrays\"]\n",
    "        }\n",
    "    \n",
    "    def _prepare_loss_input(self, batch, model_output):\n",
    "        tokenizer_output = tokenizer.batch_encode_plus([exm.upper() for exm in batch[\"transcriptions\"]], padding=True)\n",
    "        \n",
    "        return {\n",
    "            \"logits\": model_output[0], \n",
    "            \"logits_mask\": model_output[1], \n",
    "            \"tokens\": torch.tensor(tokenizer_output[\"input_ids\"]).to(\"cuda\"),\n",
    "            \"tokens_mask\": torch.tensor(tokenizer_output[\"attention_mask\"]).to(\"cuda\"), \n",
    "            \"blank\": self.tokenizer.word_delimiter_token_id\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T16:39:10.951019200Z",
     "start_time": "2024-02-01T16:39:10.947643900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "outputs": [],
   "source": [
    "from torch.optim import Adam"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T16:39:11.598180500Z",
     "start_time": "2024-02-01T16:39:11.590671700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=0.00001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T16:39:11.888260700Z",
     "start_time": "2024-02-01T16:39:11.880746400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "outputs": [],
   "source": [
    "trainer = WhisperTrainer(model, loader, optimizer, tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T16:39:12.190871Z",
     "start_time": "2024-02-01T16:39:12.183253800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 948736it [00:33, 28058.17it/s]\n",
      "C:\\Users\\znaum\\AppData\\Local\\Temp\\ipykernel_16968\\3969430525.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tokens = torch.tensor(tokens)\n",
      "C:\\Users\\znaum\\AppData\\Local\\Temp\\ipykernel_16968\\3969430525.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tokens_lengths = torch.sum(torch.tensor(tokens_mask), dim=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.3525269031524658\n",
      "loss: 0.8549197912216187\n",
      "loss: 0.9841049313545227\n",
      "loss: 0.7877734899520874\n",
      "loss: 0.4664420187473297\n",
      "loss: 0.5828015208244324\n",
      "loss: 0.7376987338066101\n",
      "loss: 0.7080963850021362\n",
      "loss: 0.3800029754638672\n",
      "loss: 0.4763419032096863\n",
      "loss: 0.6368874311447144\n",
      "loss: 0.7256090641021729\n",
      "loss: 0.6380287408828735\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[288], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtrain(\u001B[38;5;241m1\u001B[39m)\n",
      "Cell \u001B[1;32mIn[282], line 22\u001B[0m, in \u001B[0;36mTrainerBase.train\u001B[1;34m(self, epochs)\u001B[0m\n\u001B[0;32m     20\u001B[0m model_input \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_model_input(batch)\n\u001B[0;32m     21\u001B[0m model_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_input)\n\u001B[1;32m---> 22\u001B[0m loss_input \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_loss_input(batch, model_output)\n\u001B[0;32m     23\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mloss_input)\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "Cell \u001B[1;32mIn[283], line 32\u001B[0m, in \u001B[0;36mWhisperTrainer._prepare_loss_input\u001B[1;34m(self, batch, model_output)\u001B[0m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_prepare_loss_input\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch, model_output):\n\u001B[0;32m     27\u001B[0m     tokenizer_output \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mbatch_encode_plus([exm\u001B[38;5;241m.\u001B[39mupper() \u001B[38;5;28;01mfor\u001B[39;00m exm \u001B[38;5;129;01min\u001B[39;00m batch[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtranscriptions\u001B[39m\u001B[38;5;124m\"\u001B[39m]], padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     29\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {\n\u001B[0;32m     30\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogits\u001B[39m\u001B[38;5;124m\"\u001B[39m: model_output[\u001B[38;5;241m0\u001B[39m], \n\u001B[0;32m     31\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogits_mask\u001B[39m\u001B[38;5;124m\"\u001B[39m: model_output[\u001B[38;5;241m1\u001B[39m], \n\u001B[1;32m---> 32\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtokens\u001B[39m\u001B[38;5;124m\"\u001B[39m: torch\u001B[38;5;241m.\u001B[39mtensor(tokenizer_output[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m])\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m     33\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtokens_mask\u001B[39m\u001B[38;5;124m\"\u001B[39m: torch\u001B[38;5;241m.\u001B[39mtensor(tokenizer_output[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m\"\u001B[39m])\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m), \n\u001B[0;32m     34\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mblank\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer\u001B[38;5;241m.\u001B[39mword_delimiter_token_id\n\u001B[0;32m     35\u001B[0m     }\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T17:01:39.857047200Z",
     "start_time": "2024-02-01T17:00:59.475109Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
